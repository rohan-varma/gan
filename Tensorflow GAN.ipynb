{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x_image, reuse=False):\n",
    "    if (reuse):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    #First Conv and Pool Layers\n",
    "    with tf.variable_scope('conv1-variables'):\n",
    "        W_conv1 = tf.get_variable('d_wconv1', [5, 5, 1, 8], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv1 = tf.get_variable('d_bconv1', [8], initializer=tf.constant_initializer(0))\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = avg_pool_2x2(h_conv1)\n",
    "\n",
    "    #Second Conv and Pool Layers\n",
    "    \n",
    "    W_conv2 = tf.get_variable('d_wconv2', [5, 5, 8, 16], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    b_conv2 = tf.get_variable('d_bconv2', [16], initializer=tf.constant_initializer(0))\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = avg_pool_2x2(h_conv2)\n",
    "\n",
    "    #First Fully Connected Layer\n",
    "    W_fc1 = tf.get_variable('d_wfc1', [7 * 7 * 16, 32], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    b_fc1 = tf.get_variable('d_bfc1', [32], initializer=tf.constant_initializer(0))\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*16])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    #Second Fully Connected Layer\n",
    "    W_fc2 = tf.get_variable('d_wfc2', [32, 1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    b_fc2 = tf.get_variable('d_bfc2', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "    #Final Layer\n",
    "    y_conv=(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generator(z, batch_size, z_dim, reuse=False):\n",
    "    if (reuse):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    g_dim = 64 #Number of filters of first layer of generator \n",
    "    c_dim = 1 #Color dimension of output (MNIST is grayscale, so c_dim = 1 for us)\n",
    "    s = 28 #Output size of the image\n",
    "    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16) #We want to slowly upscale the image, so these values will help\n",
    "                                                              #make that change gradual.\n",
    "\n",
    "    h0 = tf.reshape(z, [batch_size, s16+1, s16+1, 25])\n",
    "    h0 = tf.nn.relu(h0)\n",
    "    #Dimensions of h0 = batch_size x 2 x 2 x 25\n",
    "\n",
    "    #First DeConv Layer\n",
    "    output1_shape = [batch_size, s8, s8, g_dim*4]\n",
    "    W_conv1 = tf.get_variable('g_wconv1', [5, 5, output1_shape[-1], int(h0.get_shape()[-1])], \n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_conv1 = tf.get_variable('g_bconv1', [output1_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "    H_conv1 = tf.nn.conv2d_transpose(h0, W_conv1, output_shape=output1_shape, \n",
    "                                     strides=[1, 2, 2, 1], padding='SAME') + b_conv1\n",
    "    H_conv1 = tf.contrib.layers.batch_norm(inputs = H_conv1, center=True, scale=True, is_training=True, scope=\"g_bn1\")\n",
    "    H_conv1 = tf.nn.relu(H_conv1)\n",
    "    #Dimensions of H_conv1 = batch_size x 3 x 3 x 256\n",
    "\n",
    "    #Second DeConv Layer\n",
    "    output2_shape = [batch_size, s4 - 1, s4 - 1, g_dim*2]\n",
    "    W_conv2 = tf.get_variable('g_wconv2', [5, 5, output2_shape[-1], int(H_conv1.get_shape()[-1])], \n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_conv2 = tf.get_variable('g_bconv2', [output2_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "    H_conv2 = tf.nn.conv2d_transpose(H_conv1, W_conv2, output_shape=output2_shape, \n",
    "                                     strides=[1, 2, 2, 1], padding='SAME') + b_conv2\n",
    "    H_conv2 = tf.contrib.layers.batch_norm(inputs = H_conv2, center=True, scale=True, is_training=True, scope=\"g_bn2\")\n",
    "    H_conv2 = tf.nn.relu(H_conv2)\n",
    "    #Dimensions of H_conv2 = batch_size x 6 x 6 x 128\n",
    "\n",
    "    #Third DeConv Layer\n",
    "    output3_shape = [batch_size, s2 - 2, s2 - 2, g_dim*1]\n",
    "    W_conv3 = tf.get_variable('g_wconv3', [5, 5, output3_shape[-1], int(H_conv2.get_shape()[-1])], \n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_conv3 = tf.get_variable('g_bconv3', [output3_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "    H_conv3 = tf.nn.conv2d_transpose(H_conv2, W_conv3, output_shape=output3_shape, \n",
    "                                     strides=[1, 2, 2, 1], padding='SAME') + b_conv3\n",
    "    H_conv3 = tf.contrib.layers.batch_norm(inputs = H_conv3, center=True, scale=True, is_training=True, scope=\"g_bn3\")\n",
    "    H_conv3 = tf.nn.relu(H_conv3)\n",
    "    #Dimensions of H_conv3 = batch_size x 12 x 12 x 64\n",
    "\n",
    "    #Fourth DeConv Layer\n",
    "    output4_shape = [batch_size, s, s, c_dim]\n",
    "    W_conv4 = tf.get_variable('g_wconv4', [5, 5, output4_shape[-1], int(H_conv3.get_shape()[-1])], \n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_conv4 = tf.get_variable('g_bconv4', [output4_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "    H_conv4 = tf.nn.conv2d_transpose(H_conv3, W_conv4, output_shape=output4_shape, \n",
    "                                     strides=[1, 2, 2, 1], padding='VALID') + b_conv4\n",
    "    H_conv4 = tf.nn.tanh(H_conv4)\n",
    "    #Dimensions of H_conv4 = batch_size x 28 x 28 x 1\n",
    "\n",
    "    return H_conv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def show_generated_image():\n",
    "    z_dim = 100\n",
    "    z_holder = tf.placeholder(tf.float32, [None, z_dim])\n",
    "    sample_img = generator(z_holder, 1, z_dim)\n",
    "    test_z = np.random.normal(-1, 1, [1, z_dim])\n",
    "    img = sample_img.eval(feed_dict = {z_holder: test_z})\n",
    "    img = img.squeeze()\n",
    "    plt.imshow(img, cmap = 'gray_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "z_dimensions = 100\n",
    "\n",
    "x_placeholder = tf.placeholder(\"float\", shape = [None,28,28,1]) #Placeholder for input images to the discriminator\n",
    "z_placeholder = tf.placeholder(tf.float32, [None, z_dimensions]) #Placeholder for input noise vectors to the generator\n",
    "Dx = discriminator(x_placeholder) #Dx will hold discriminator prediction probabilities for the real MNIST images\n",
    "Gz = generator(z_placeholder, batch_size, z_dimensions) #Gz holds the generated images\n",
    "Dg = discriminator(Gz, reuse=True) #Dg will hold discriminator prediction probabilities for generated images\n",
    "\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.ones_like(Dg))) # ensure forward compatibility: function needs to have logits and labels args explicitly used\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dx, labels = tf.ones_like(Dx)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = Dg, labels = tf.zeros_like(Dg)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "d_vars = [var for var in tvars if 'd_' in var.name]\n",
    "g_vars = [var for var in tvars if 'g_' in var.name]\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope(),reuse=False): \n",
    "    trainerD = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
    "    trainerG = tf.train.AdamOptimizer().minimize(g_loss, var_list=g_vars)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "epochs = 10000\n",
    "for i in range(epochs):\n",
    "    z_batch = np.random.normal(-1, 1, size=[batch_size, z_dimensions])\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    real_image_batch = np.reshape(real_image_batch[0],[batch_size,28,28,1])\n",
    "    _,dLoss = sess.run([trainerD, d_loss],feed_dict={z_placeholder:z_batch,x_placeholder:real_image_batch}) #Update the discriminator\n",
    "    _,gLoss = sess.run([trainerG,g_loss],feed_dict={z_placeholder:z_batch}) #Update the generator \n",
    "    if i % 100 == 0:\n",
    "        print(\"discriminator loss: {}\".format(dLoss))\n",
    "        print(\"generator loss: {}\".format(gLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_generated_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
